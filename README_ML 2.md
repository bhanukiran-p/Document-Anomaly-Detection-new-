# Machine Learning in XFORIA DAD

**Complete Guide to ML Fraud Detection Implementation**

---

## ğŸ“‹ Table of Contents

1. [ML Architecture Overview](#ml-architecture-overview)
2. [Feature Engineering](#feature-engineering)
3. [Model Training Pipeline](#model-training-pipeline)
4. [Inference Process](#inference-process)
5. [Ensemble Methodology](#ensemble-methodology)
6. [Model Evaluation](#model-evaluation)
7. [Integration with System](#integration-with-system)
8. [Model Files & Structure](#model-files--structure)

---

## ğŸ—ï¸ ML Architecture Overview

### High-Level ML Flow

```
Document Upload
    â†“
OCR Extraction (Mindee API)
    â†“
Data Normalization
    â†“
Feature Extraction (30-50 features)
    â†“
Feature Scaling (StandardScaler)
    â†“
ML Ensemble Prediction
    â”œâ”€ Random Forest Model â†’ rf_score
    â””â”€ XGBoost Model â†’ xgb_score
    â†“
Ensemble Score = (0.4 Ã— rf_score) + (0.6 Ã— xgb_score)
    â†“
Validation Rule Adjustments
    â†“
Final Fraud Risk Score (0.0 - 1.0)
    â†“
Risk Level Classification (LOW/MEDIUM/HIGH/CRITICAL)
```

### Model Architecture Summary

| Document Type | Model Architecture | Ensemble Weighting | Feature Count | Model Type |
|--------------|-------------------|-------------------|---------------|------------|
| **Checks** | Random Forest + XGBoost | 40% RF + 60% XGB | 30 | Regressor |
| **Paystubs** | Random Forest + XGBoost | 40% RF + 60% XGB | 18 | Regressor |
| **Money Orders** | Random Forest + XGBoost | 40% RF + 60% XGB | 30 | Regressor |
| **Bank Statements** | Random Forest + XGBoost | 40% RF + 60% XGB | 35 | Regressor |
| **Real-Time Transactions** | XGBoost (single) | N/A | 50+ | Regressor |

**Key Design Decisions:**
- **Ensemble Approach**: Combines Random Forest (robust, handles non-linear) + XGBoost (high performance, gradient boosting)
- **Regressor Models**: Predict continuous fraud risk score (0-100) instead of binary classification
- **Weighted Ensemble**: 40% RF + 60% XGB (XGBoost weighted higher due to better performance)
- **Feature Scaling**: StandardScaler applied before prediction (mean=0, std=1)

---

## ğŸ”§ Feature Engineering

### Feature Extraction Process

**Location**: `Backend/{document_type}/ml/{document_type}_feature_extractor.py`

**Example**: `Backend/check/ml/check_feature_extractor.py`

### Feature Categories

#### 1. Basic Document Features (Checks - Features 1-15)

```python
# Feature 1: Bank validity (0.0 or 1.0)
bank_validity = 1.0 if bank_name in supported_banks else 0.0

# Feature 2: Routing number validity (0.0 or 1.0)
routing_validity = 1.0 if routing_number matches 9-digit pattern else 0.0

# Feature 3: Account number present (0.0 or 1.0)
account_present = 1.0 if account_number else 0.0

# Feature 4: Check number validity (0.0 or 1.0)
check_number_valid = 1.0 if check_number matches pattern else 0.0

# Feature 5: Amount (numeric value, capped at 50,000)
amount_numeric = min(amount, 50000.0) if amount > 0 else 0.0

# Feature 6: Amount category (0-4)
# 0 = $0-100, 1 = $100-1000, 2 = $1000-5000, 3 = $5000-10000, 4 = $10000+
amount_category = categorize_amount(amount_numeric)

# Feature 7: Amount is round number (0.0 or 1.0)
amount_is_round = 1.0 if amount ends in .00 else 0.0

# Feature 8: Payer name present (0.0 or 1.0)
payer_present = 1.0 if payer_name else 0.0

# Feature 9: Payee name present (0.0 or 1.0)
payee_present = 1.0 if payee_name else 0.0

# Feature 10: Payee address present (0.0 or 1.0)
payee_address = 1.0 if payee_address else 0.0

# Feature 11: Date present (0.0 or 1.0)
date_present = 1.0 if check_date else 0.0

# Feature 12: Future date (0.0 or 1.0)
future_date = 1.0 if check_date > today else 0.0

# Feature 13: Date age in days (0-365+)
date_age_days = (today - check_date).days if check_date else 0.0

# Feature 14: Signature detected (0.0 or 1.0)
signature_detected = 1.0 if signature_present else 0.0

# Feature 15: Memo present (0.0 or 1.0)
memo = 1.0 if memo_text else 0.0
```

#### 2. Advanced Validation Features (Checks - Features 16-30)

```python
# Feature 16: Exact amount match (0.0 or 1.0)
# Numeric amount matches written amount
exact_amount_match = 1.0 if numeric_amount == written_amount else 0.0

# Feature 17: Amount parsing confidence (0.0-1.0)
amount_parsing_confidence = OCR_confidence_for_amount

# Feature 18: Suspicious amount pattern (0.0 or 1.0)
# Amounts like $999, $9999 (just below limits)
suspicious_amount = 1.0 if amount in suspicious_patterns else 0.0

# Feature 19: Date format consistency (0.0 or 1.0)
date_format_consistency = 1.0 if date_format_valid else 0.0

# Feature 20: Weekend/holiday check (0.0 or 1.0)
weekend_holiday = 1.0 if check_date is weekend/holiday else 0.0

# Feature 21: Critical missing fields count (0-10)
critical_missing_count = count(missing_critical_fields)

# Feature 22: Field quality score (0.0-1.0)
field_quality_score = completeness_score

# Feature 23: Bank-routing match (0.0 or 1.0)
bank_routing_match = 1.0 if routing_number matches bank else 0.0

# Feature 24: Check number pattern (0.0-1.0)
check_number_pattern = pattern_validity_score

# Feature 25: Address validity (0.0 or 1.0)
address_valid = 1.0 if address_format_valid else 0.0

# Feature 26: Name consistency (0.0-1.0)
name_consistency = similarity_score(payer_name, payee_name)

# Feature 27: Signature required (0.0 or 1.0)
signature_required = 1.0  # Always required for checks

# Feature 28: Check type risk (0.0-1.0)
check_type_risk = risk_score_based_on_check_type

# Feature 29: Text quality score (0.0-1.0)
text_quality_score = OCR_confidence_score

# Feature 30: Endorsement present (0.0 or 1.0)
endorsement = 1.0 if endorsement_detected else 0.0
```

### Feature Extraction Code Example

```python
class CheckFeatureExtractor:
    def extract_features(self, extracted_data: Dict, raw_text: str = "") -> List[float]:
        """
        Extract 30 features from check data
        
        Returns:
            List of 30 float values representing features
        """
        features = []
        
        # Get fields from data
        bank_name = extracted_data.get('bank_name')
        amount_numeric = self._extract_numeric_amount(extracted_data.get('amount'))
        signature_detected = extracted_data.get('signature_detected', False)
        
        # Feature 1: Bank validity
        features.append(1.0 if self._is_bank_supported(bank_name) else 0.0)
        
        # Feature 5: Amount (capped at 50,000)
        features.append(min(amount_numeric, 50000.0) if amount_numeric > 0 else 0.0)
        
        # Feature 14: Signature detected
        features.append(1.0 if signature_detected else 0.0)
        
        # ... extract all 30 features
        
        return features  # Returns list of 30 floats
```

### Feature Scaling

**StandardScaler** is applied to all features before prediction:

```python
from sklearn.preprocessing import StandardScaler

# During training
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# During inference
X_scaled = scaler.transform([features])
```

**Why StandardScaler?**
- Normalizes features to mean=0, std=1
- Required for XGBoost (sensitive to feature scales)
- Improves Random Forest performance
- Prevents features with larger ranges from dominating

---

## ğŸ“ Model Training Pipeline

### Training Script Location

**Checks**: `Backend/training/retrain_check_models_30features.py`  
**Paystubs**: `Backend/training/train_paystub_models.py`  
**Money Orders**: `Backend/training/train_money_order_models.py`  
**Bank Statements**: `Backend/training/train_risk_model.py`

### Training Process

#### Step 1: Generate Synthetic Training Data

```python
def generate_training_data(n_samples=2000):
    """
    Generate synthetic training data with known fraud patterns
    
    Returns:
        DataFrame with features + risk_score (0-100)
    """
    data = []
    for i in range(n_samples):
        # Generate legitimate or fraudulent sample
        is_legit = random.random() < 0.7  # 70% legitimate, 30% fraudulent
        
        if is_legit:
            features = generate_legitimate_check()
        else:
            features = generate_fraudulent_check()
        
        # Calculate ground truth risk score
        risk_score = calculate_risk_score(features)
        data.append(features + [risk_score])
    
    return pd.DataFrame(data, columns=feature_names + ['risk_score'])
```

**Synthetic Data Generation:**
- **Legitimate Checks**: Valid bank, routing number, signature present, normal amounts
- **Fraudulent Checks**: Invalid bank, missing signature, amount mismatch, future dates
- **Risk Score Calculation**: Based on fraud indicators (missing signature = +50, invalid bank = +25, etc.)

#### Step 2: Train/Test Split

```python
from sklearn.model_selection import train_test_split

X = df.drop('risk_score', axis=1).values
y = df['risk_score'].values  # Target: 0-100 fraud risk score

X_train, X_test, y_train, y_test = train_test_split(
    X, y, test_size=0.2, random_state=42
)
```

**Split Ratio**: 80% training, 20% testing

#### Step 3: Feature Scaling

```python
from sklearn.preprocessing import StandardScaler

scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# Save scaler for inference
joblib.dump(scaler, 'check_feature_scaler.pkl')
```

#### Step 4: Train Random Forest

```python
from sklearn.ensemble import RandomForestRegressor

rf_model = RandomForestRegressor(
    n_estimators=200,      # Number of trees
    max_depth=15,          # Maximum tree depth
    min_samples_split=5,   # Minimum samples to split node
    min_samples_leaf=2,    # Minimum samples in leaf
    random_state=42,
    n_jobs=-1              # Use all CPU cores
)

rf_model.fit(X_train_scaled, y_train)

# Evaluate
rf_pred = rf_model.predict(X_test_scaled)
mse = mean_squared_error(y_test, rf_pred)
r2 = r2_score(y_test, rf_pred)

print(f"RF Test MSE: {mse:.4f}")
print(f"RF Test RÂ²:  {r2:.4f}")

# Save model
joblib.dump(rf_model, 'check_random_forest.pkl')
```

**Random Forest Hyperparameters:**
- `n_estimators=200`: 200 decision trees
- `max_depth=15`: Prevents overfitting
- `min_samples_split=5`: Requires 5 samples to split
- `min_samples_leaf=2`: Minimum 2 samples in leaf nodes

#### Step 5: Train XGBoost

```python
import xgboost as xgb

xgb_model = xgb.XGBRegressor(
    n_estimators=200,        # Number of boosting rounds
    max_depth=8,             # Maximum tree depth
    learning_rate=0.1,        # Step size shrinkage
    subsample=0.8,           # Row subsampling ratio
    colsample_bytree=0.8,    # Column subsampling ratio
    random_state=42,
    n_jobs=-1
)

xgb_model.fit(X_train_scaled, y_train)

# Evaluate
xgb_pred = xgb_model.predict(X_test_scaled)
mse = mean_squared_error(y_test, xgb_pred)
r2 = r2_score(y_test, xgb_pred)

print(f"XGB Test MSE: {mse:.4f}")
print(f"XGB Test RÂ²:  {r2:.4f}")

# Save model
joblib.dump(xgb_model, 'check_xgboost.pkl')
```

**XGBoost Hyperparameters:**
- `n_estimators=200`: 200 boosting rounds
- `max_depth=8`: Tree depth (shallower than RF)
- `learning_rate=0.1`: Shrinkage factor (prevents overfitting)
- `subsample=0.8`: 80% of rows used per tree
- `colsample_bytree=0.8`: 80% of features used per tree

#### Step 6: Model Evaluation

```python
# Ensemble prediction
ensemble_pred = (0.4 * rf_pred) + (0.6 * xgb_pred)

# Evaluate ensemble
ensemble_mse = mean_squared_error(y_test, ensemble_pred)
ensemble_r2 = r2_score(y_test, ensemble_pred)

print(f"Ensemble Test MSE: {ensemble_mse:.4f}")
print(f"Ensemble Test RÂ²:  {ensemble_r2:.4f}")
```

**Evaluation Metrics:**
- **MSE (Mean Squared Error)**: Lower is better (measures prediction error)
- **RÂ² Score**: Higher is better (measures explained variance, 0-1 scale)

**Expected Performance:**
- **RÂ² Score**: ~0.85-0.90 (85-90% variance explained)
- **MSE**: ~50-100 (average error of 7-10 points on 0-100 scale)

#### Step 7: Save Models

```python
import joblib

# Save models
joblib.dump(rf_model, 'check_random_forest.pkl')
joblib.dump(xgb_model, 'check_xgboost.pkl')
joblib.dump(scaler, 'check_feature_scaler.pkl')

# Save metadata
metadata = {
    'model_type': 'ensemble_rf_xgb',
    'feature_count': 30,
    'trained_at': datetime.now().isoformat(),
    'rf_params': rf_model.get_params(),
    'xgb_params': xgb_model.get_params(),
    'test_mse': ensemble_mse,
    'test_r2': ensemble_r2
}

with open('check_model_metadata.json', 'w') as f:
    json.dump(metadata, f, indent=2)
```

---

## ğŸ”® Inference Process

### Prediction Flow

**Location**: `Backend/{document_type}/ml/{document_type}_fraud_detector.py`

**Example**: `Backend/check/ml/check_fraud_detector.py`

### Step-by-Step Inference

#### Step 1: Load Models

```python
import joblib

class CheckFraudDetector:
    def __init__(self, model_dir='models'):
        # Load models from disk
        self.rf_model = joblib.load('check_random_forest.pkl')
        self.xgb_model = joblib.load('check_xgboost.pkl')
        self.scaler = joblib.load('check_feature_scaler.pkl')
```

#### Step 2: Extract Features

```python
from check.ml.check_feature_extractor import CheckFeatureExtractor

feature_extractor = CheckFeatureExtractor()
features = feature_extractor.extract_features(extracted_data, raw_text)
# Returns: List of 30 floats
```

#### Step 3: Scale Features

```python
import numpy as np

# Convert to numpy array
X = np.array([features])  # Shape: (1, 30)

# Scale features
X_scaled = self.scaler.transform(X)  # Mean=0, Std=1
```

#### Step 4: Run Predictions

```python
# Random Forest prediction
rf_pred = self.rf_model.predict(X_scaled)[0]  # Returns 0-100
rf_score = rf_pred / 100.0  # Normalize to 0-1

# XGBoost prediction
xgb_pred = self.xgb_model.predict(X_scaled)[0]  # Returns 0-100
xgb_score = xgb_pred / 100.0  # Normalize to 0-1
```

**Note**: Models are regressors, so they predict continuous values (0-100). These are normalized to 0-1 for consistency.

#### Step 5: Calculate Ensemble Score

```python
# Weighted ensemble
ensemble_score = (0.4 * rf_score) + (0.6 * xgb_score)
```

**Why 40/60 Weighting?**
- XGBoost typically performs better (higher RÂ² score)
- Random Forest provides robustness and handles outliers
- Weighted average balances both strengths

#### Step 6: Apply Validation Rule Adjustments

```python
def _apply_validation_rules(self, ensemble_score, check_data, features):
    """
    Adjust score based on validation rules
    """
    adjusted_score = ensemble_score
    
    # Missing signature adds 0.3 to score
    if not check_data.get('signature_detected'):
        adjusted_score = min(1.0, adjusted_score + 0.3)
    
    # Duplicate check = automatic reject
    if self._is_duplicate(check_data):
        adjusted_score = 1.0
    
    # Critical validation issues add 0.2
    if self._has_critical_issues(check_data):
        adjusted_score = min(1.0, adjusted_score + 0.2)
    
    return adjusted_score
```

#### Step 7: Determine Risk Level

```python
def _determine_risk_level(self, score):
    """
    Classify risk level based on score
    """
    if score >= 0.7:
        return 'CRITICAL'
    elif score >= 0.5:
        return 'HIGH'
    elif score >= 0.3:
        return 'MEDIUM'
    else:
        return 'LOW'
```

#### Step 8: Generate Anomalies

```python
def _generate_anomalies(self, check_data, features, feature_names, score):
    """
    Generate list of anomalies based on features
    """
    anomalies = []
    
    # Check feature values
    if features[0] == 0.0:  # Invalid bank
        anomalies.append("Unsupported bank detected")
    
    if features[13] == 0.0:  # Missing signature
        anomalies.append("Missing signature")
    
    if features[15] == 0.0:  # Amount mismatch
        anomalies.append("Numeric and written amounts don't match")
    
    return anomalies
```

### Complete Inference Code Example

```python
def predict_fraud(self, check_data: Dict, raw_text: str = "") -> Dict:
    """
    Complete inference pipeline
    """
    # 1. Extract features
    features = self.feature_extractor.extract_features(check_data, raw_text)
    
    # 2. Scale features
    X = np.array([features])
    X_scaled = self.scaler.transform(X)
    
    # 3. Run predictions
    rf_pred = self.rf_model.predict(X_scaled)[0]
    rf_score = max(0.0, min(1.0, rf_pred / 100.0))
    
    xgb_pred = self.xgb_model.predict(X_scaled)[0]
    xgb_score = max(0.0, min(1.0, xgb_pred / 100.0))
    
    # 4. Ensemble
    ensemble_score = (0.4 * rf_score) + (0.6 * xgb_score)
    
    # 5. Apply validation rules
    adjusted_score = self._apply_validation_rules(ensemble_score, check_data, features)
    
    # 6. Determine risk level
    risk_level = self._determine_risk_level(adjusted_score)
    
    # 7. Generate anomalies
    anomalies = self._generate_anomalies(check_data, features, feature_names, adjusted_score)
    
    return {
        'fraud_risk_score': round(adjusted_score, 4),
        'risk_level': risk_level,
        'model_confidence': round(max(rf_score, xgb_score), 4),
        'model_scores': {
            'random_forest': round(rf_score, 4),
            'xgboost': round(xgb_score, 4),
            'ensemble': round(ensemble_score, 4),
            'adjusted': round(adjusted_score, 4)
        },
        'anomalies': anomalies
    }
```

---

## ğŸ¯ Ensemble Methodology

### Why Ensemble?

**Advantages:**
1. **Reduced Overfitting**: Multiple models reduce risk of overfitting
2. **Better Generalization**: Combines strengths of different algorithms
3. **Robustness**: Less sensitive to outliers and noise
4. **Higher Accuracy**: Typically outperforms single models

### Ensemble Architecture

```
Input Features (30 floats)
    â†“
    â”œâ”€â†’ Random Forest Model â†’ rf_score (0-1)
    â”‚   â””â”€ 200 trees, max_depth=15
    â”‚
    â””â”€â†’ XGBoost Model â†’ xgb_score (0-1)
        â””â”€ 200 rounds, max_depth=8, learning_rate=0.1
    â†“
Ensemble Score = (0.4 Ã— rf_score) + (0.6 Ã— xgb_score)
    â†“
Validation Rule Adjustments
    â†“
Final Fraud Risk Score
```

### Weight Selection (40/60)

**Rationale:**
- **XGBoost (60%)**: Typically achieves higher RÂ² scores (~0.90 vs ~0.85)
- **Random Forest (40%)**: Provides robustness and handles non-linear patterns
- **Empirical Testing**: 40/60 weighting showed best performance on validation set

**Alternative Weightings Tested:**
- 50/50: Balanced but lower overall performance
- 30/70: Too much weight on XGBoost, loses RF robustness
- 40/60: **Optimal** - best balance of accuracy and robustness

### Model Complementarity

**Random Forest Strengths:**
- Handles non-linear relationships well
- Robust to outliers
- Provides feature importance insights
- Less prone to overfitting with proper depth limits

**XGBoost Strengths:**
- High predictive accuracy
- Handles feature interactions automatically
- Gradient boosting captures complex patterns
- Efficient training and prediction

**Combined Benefits:**
- RF catches patterns XGBoost might miss
- XGBoost captures complex interactions RF might miss
- Ensemble reduces variance and improves generalization

---

## ğŸ“Š Model Evaluation

### Evaluation Metrics

#### Regression Metrics (Models predict 0-100 risk score)

**Mean Squared Error (MSE):**
```python
mse = mean_squared_error(y_true, y_pred)
# Lower is better
# Typical values: 50-100 (error of 7-10 points)
```

**RÂ² Score (Coefficient of Determination):**
```python
r2 = r2_score(y_true, y_pred)
# Higher is better (0-1 scale)
# Typical values: 0.85-0.90 (85-90% variance explained)
```

**Mean Absolute Error (MAE):**
```python
mae = mean_absolute_error(y_true, y_pred)
# Lower is better
# Typical values: 6-8 points on 0-100 scale
```

### Model Performance Benchmarks

| Document Type | RF RÂ² | XGB RÂ² | Ensemble RÂ² | Ensemble MSE |
|--------------|-------|--------|-------------|--------------|
| **Checks** | ~0.85 | ~0.90 | ~0.92 | ~50 |
| **Paystubs** | ~0.82 | ~0.88 | ~0.90 | ~60 |
| **Money Orders** | ~0.84 | ~0.89 | ~0.91 | ~55 |
| **Bank Statements** | ~0.83 | ~0.87 | ~0.89 | ~65 |

### Feature Importance Analysis

**Random Forest Feature Importance:**
```python
# Get feature importance
feature_importance = rf_model.feature_importances_

# Top 5 most important features (example for checks):
# 1. signature_detected (0.15) - Missing signature is strong fraud indicator
# 2. exact_amount_match (0.12) - Amount mismatch indicates tampering
# 3. bank_validity (0.10) - Invalid bank suggests fraud
# 4. date_age_days (0.09) - Stale checks are suspicious
# 5. critical_missing_count (0.08) - Missing fields reduce trust
```

**XGBoost Feature Importance:**
```python
# Get feature importance
feature_importance = xgb_model.feature_importances_

# Similar ranking but may differ slightly
# XGBoost often emphasizes interaction features more
```

### Cross-Validation

**K-Fold Cross-Validation (Optional):**
```python
from sklearn.model_selection import cross_val_score

# 5-fold cross-validation
rf_scores = cross_val_score(rf_model, X_train_scaled, y_train, cv=5, scoring='r2')
xgb_scores = cross_val_score(xgb_model, X_train_scaled, y_train, cv=5, scoring='r2')

print(f"RF CV RÂ²: {rf_scores.mean():.4f} (+/- {rf_scores.std() * 2:.4f})")
print(f"XGB CV RÂ²: {xgb_scores.mean():.4f} (+/- {xgb_scores.std() * 2:.4f})")
```

---

## ğŸ”— Integration with System

### ML Integration Flow

```
Document Analysis Pipeline
    â†“
Stage 1: OCR Extraction (Mindee)
    â†“
Stage 2: Normalization
    â†“
Stage 3: Validation Rules
    â†“
Stage 4: ML Fraud Detection â† ML INTEGRATION POINT
    â”œâ”€ Feature Extraction
    â”œâ”€ Feature Scaling
    â”œâ”€ Ensemble Prediction
    â””â”€ Risk Level Classification
    â†“
Stage 5: Customer History
    â†“
Stage 6: AI Analysis (uses ML scores)
    â†“
Stage 7-10: Final Decision
```

### ML Detector Initialization

**Location**: `Backend/{document_type}/{document_type}_extractor.py`

```python
class CheckExtractor:
    def __init__(self):
        # Initialize ML detector
        from check.ml.check_fraud_detector import CheckFraudDetector
        self.ml_detector = CheckFraudDetector(model_dir='models')
        
        # Initialize AI agent (uses ML scores)
        from check.ai.check_fraud_analysis_agent import CheckFraudAnalysisAgent
        self.ai_agent = CheckFraudAnalysisAgent(...)
```

### ML Prediction Call

```python
def extract_and_analyze(self, image_path: str) -> Dict:
    # ... OCR and normalization ...
    
    # Stage 4: ML Fraud Detection
    ml_analysis = self._run_ml_fraud_detection(normalized_data, raw_text)
    # Returns: {
    #   'fraud_risk_score': 0.75,
    #   'risk_level': 'HIGH',
    #   'model_confidence': 0.89,
    #   'model_scores': {...},
    #   'anomalies': [...]
    # }
    
    # Stage 6: AI Analysis (uses ML scores)
    ai_analysis = self._run_ai_analysis(
        normalized_data, 
        ml_analysis,  # ML scores passed to AI
        customer_info
    )
    
    # ... final decision ...
```

### ML Scores Used by AI

The AI agent receives ML analysis and uses it to make recommendations:

```python
# AI prompt includes ML scores
prompt = f"""
ML Model Analysis:
- Fraud Risk Score: {ml_analysis['fraud_risk_score']:.1%}
- Risk Level: {ml_analysis['risk_level']}
- Model Confidence: {ml_analysis['model_confidence']:.1%}
- Anomalies: {ml_analysis['anomalies']}

Based on this ML analysis and the document data, provide your recommendation.
"""
```

---

## ğŸ“ Model Files & Structure

### File Structure

```
Backend/
â”œâ”€â”€ check/
â”‚   â””â”€â”€ ml/
â”‚       â”œâ”€â”€ check_feature_extractor.py
â”‚       â”œâ”€â”€ check_fraud_detector.py
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ check_random_forest.pkl
â”‚           â”œâ”€â”€ check_xgboost.pkl
â”‚           â”œâ”€â”€ check_feature_scaler.pkl
â”‚           â””â”€â”€ ACTIVE_VERSION.txt
â”œâ”€â”€ paystub/
â”‚   â””â”€â”€ ml/
â”‚       â”œâ”€â”€ paystub_feature_extractor.py
â”‚       â”œâ”€â”€ paystub_fraud_detector.py
â”‚       â””â”€â”€ models/
â”‚           â”œâ”€â”€ paystub_random_forest.pkl
â”‚           â”œâ”€â”€ paystub_xgboost.pkl
â”‚           â”œâ”€â”€ paystub_feature_scaler.pkl
â”‚           â””â”€â”€ paystub_model_metadata_latest.json
â””â”€â”€ training/
    â”œâ”€â”€ retrain_check_models_30features.py
    â”œâ”€â”€ train_paystub_models.py
    â”œâ”€â”€ train_money_order_models.py
    â””â”€â”€ train_risk_model.py
```

### Model File Formats

**`.pkl` Files (Pickle):**
- Serialized Python objects using `joblib`
- Contains trained model objects (RF/XGBoost)
- Contains `StandardScaler` object
- Loaded with `joblib.load()`

**`.json` Files (Metadata):**
```json
{
  "model_type": "ensemble_rf_xgb",
  "feature_count": 30,
  "trained_at": "2024-12-08T10:30:00",
  "rf_params": {...},
  "xgb_params": {...},
  "test_mse": 52.34,
  "test_r2": 0.89
}
```

### Model Versioning

**`ACTIVE_VERSION.txt`:**
```
Model Version: 1.0
Trained: 2024-12-08
Features: 30
RF RÂ²: 0.85
XGB RÂ²: 0.90
Ensemble RÂ²: 0.92
```

---

## ğŸš€ Training Commands

### Train All Models

```bash
# Train Check Models
cd Backend
python training/retrain_check_models_30features.py

# Train Paystub Models
python training/train_paystub_models.py

# Train Money Order Models
python training/train_money_order_models.py

# Train Bank Statement Models
python training/train_risk_model.py
```

### Expected Output

```
Generating 2000 check samples with 30 features...
Generated 2000 samples
Risk: min=0.0, max=100.0, mean=35.2

Scaling features...

============================================================
Training Random Forest (30 features)...
============================================================
Test MSE: 48.23
Test RÂ²:  0.89

============================================================
Training XGBoost (30 features)...
============================================================
Test MSE: 42.15
Test RÂ²:  0.92

Ensemble Test MSE: 40.87
Ensemble Test RÂ²:  0.93

Models saved successfully!
```

---

## ğŸ” Troubleshooting

### Common Issues

**1. Models Not Loading:**
```python
# Error: FileNotFoundError: check_random_forest.pkl
# Solution: Train models first
python Backend/training/retrain_check_models_30features.py
```

**2. Feature Count Mismatch:**
```python
# Error: Expected 30 features, got 18
# Solution: Ensure feature extractor matches model training
# Check: feature_extractor.extract_features() returns correct count
```

**3. Scaler Not Found:**
```python
# Error: check_feature_scaler.pkl not found
# Solution: Retrain models (scaler is saved during training)
python Backend/training/retrain_check_models_30features.py
```

**4. Low Model Confidence:**
```python
# Issue: model_confidence < 0.5
# Causes:
# - Models not properly trained
# - Feature extraction errors
# - Data quality issues
# Solution: Retrain models, check feature extraction
```

---

## ğŸ“š Key Takeaways

1. **Ensemble Approach**: RF + XGBoost (40/60) provides best accuracy
2. **Feature Engineering**: 30-50 features capture fraud patterns
3. **StandardScaler**: Required for consistent predictions
4. **Regressor Models**: Predict continuous risk scores (0-100)
5. **Validation Rules**: Post-ML adjustments improve accuracy
6. **Integration**: ML scores feed into AI analysis for final decision

---

**Version:** 1.0.0 | **Last Updated:** December 2024


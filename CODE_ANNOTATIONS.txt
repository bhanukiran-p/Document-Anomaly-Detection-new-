================================================================================
                 XFORIA DAD - COMPREHENSIVE CODE ANNOTATIONS
                    Complete Technical Analysis & Documentation
================================================================================

PROJECT: XFORIA Document Anomaly Detection (DAD)
PURPOSE: AI-Powered Fraud Detection for Financial Documents
ARCHITECTURE: Modular Full-Stack Application with ML/AI Pipeline

================================================================================
                        TABLE OF CONTENTS
================================================================================

1. SYSTEM ARCHITECTURE OVERVIEW
2. BACKEND ARCHITECTURE & COMPONENTS
3. FRONTEND ARCHITECTURE & COMPONENTS
4. DATA FLOW & PROCESSING PIPELINE
5. ML/AI INTEGRATION DETAILS
6. DATABASE SCHEMA & OPERATIONS
7. API ENDPOINTS & REQUEST/RESPONSE FORMATS
8. SECURITY & AUTHENTICATION
9. ERROR HANDLING & LOGGING
10. PERFORMANCE OPTIMIZATIONS
11. CODE ORGANIZATION & BEST PRACTICES

================================================================================
                    1. SYSTEM ARCHITECTURE OVERVIEW
================================================================================

HIGH-LEVEL SYSTEM DESIGN:
-------------------------

┌──────────────────────────────────────────────────────────────────────┐
│                         CLIENT LAYER (React)                          │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐    │
│  │  Analysis  │  │  Insights  │  │ Real-Time  │  │    Auth    │    │
│  │   Pages    │  │Dashboards  │  │ Monitoring │  │   Pages    │    │
│  └────────────┘  └────────────┘  └────────────┘  └────────────┘    │
└──────────────────────────────────────────────────────────────────────┘
                              ↕ HTTP/REST API
┌──────────────────────────────────────────────────────────────────────┐
│                      APPLICATION LAYER (Flask)                        │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐    │
│  │   Check    │  │  Paystub   │  │Money Order │  │Bank Stmt   │    │
│  │  Analyzer  │  │  Analyzer  │  │  Analyzer  │  │  Analyzer  │    │
│  └────────────┘  └────────────┘  └────────────┘  └────────────┘    │
│                                                                       │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐    │
│  │    OCR     │  │     ML     │  │     AI     │  │  Database  │    │
│  │ Extraction │  │   Models   │  │   Agents   │  │  Storage   │    │
│  └────────────┘  └────────────┘  └────────────┘  └────────────┘    │
└──────────────────────────────────────────────────────────────────────┘
                              ↕ API Calls
┌──────────────────────────────────────────────────────────────────────┐
│                      EXTERNAL SERVICES LAYER                          │
│  ┌────────────┐  ┌────────────┐  ┌────────────┐  ┌────────────┐    │
│  │   Mindee   │  │  OpenAI    │  │  Supabase  │  │   Google   │    │
│  │    OCR     │  │   GPT-4    │  │PostgreSQL  │  │   Vision   │    │
│  └────────────┘  └────────────┘  └────────────┘  └────────────┘    │
└──────────────────────────────────────────────────────────────────────┘

TECHNOLOGY STACK:
-----------------
Frontend:  React 18.2.0, React Router 6.20.0, Recharts, ECharts
Backend:   Flask 3.0.0, Python 3.12+
ML:        XGBoost 2.0.3, scikit-learn 1.4.2, imbalanced-learn
AI:        OpenAI GPT-4, LangChain 0.1.0
OCR:       Mindee API 4.31.0+, Google Cloud Vision
Database:  Supabase (PostgreSQL)
Auth:      JWT (PyJWT 2.10.1)

DESIGN PATTERNS:
----------------
1. Modular Architecture - Each document type has isolated modules
2. Factory Pattern - Normalizer factories for bank-specific processing
3. Pipeline Pattern - Sequential processing stages (OCR → ML → AI)
4. Repository Pattern - Database access through storage classes
5. Strategy Pattern - Different fraud detection strategies per document type

================================================================================
                    2. BACKEND ARCHITECTURE & COMPONENTS
================================================================================

2.1 MAIN API SERVER (api_server.py)
------------------------------------

FILE: Backend/api_server.py
LINES: 2085 lines
PURPOSE: Central Flask application serving all API endpoints

KEY COMPONENTS:

1. Flask App Initialization:
   - CORS enabled for React frontend (port 3002)
   - Rotating file logging (10MB max, 5 backups)
   - Environment variable loading via python-dotenv
   - Health check endpoints

2. Document Analysis Endpoints:
   POST /api/check/analyze
   POST /api/paystub/analyze
   POST /api/money-order/analyze
   POST /api/bank-statement/analyze

   FLOW FOR EACH ENDPOINT:
   a) Receive multipart/form-data with file upload
   b) Validate file type (PDF/image)
   c) Save to temp_uploads/ directory
   d) Call document-specific extractor
   e) Store results in Supabase
   f) Return JSON response with analysis

3. Data Retrieval Endpoints:
   GET /api/checks/list?date_filter=last_30
   GET /api/paystubs/list?date_filter=last_30
   GET /api/money-orders/list?date_filter=last_30
   GET /api/bank-statements/list?start_date=YYYY-MM-DD&end_date=YYYY-MM-DD

   FEATURES:
   - Pagination (1000 records per page)
   - Date filtering (last_30, last_60, last_90, older, custom range)
   - Search functionality
   - Bank filtering (for bank statements)

4. Authentication Endpoints:
   POST /api/auth/register
   POST /api/auth/login

   IMPLEMENTATION:
   - JWT token generation
   - Password hashing with bcrypt
   - Token verification middleware

ANNOTATIONS:

Lines 1-100: Imports, logging setup, environment configuration
Lines 101-200: Flask app initialization, CORS, upload folder setup
Lines 201-400: Check analysis endpoint implementation
Lines 401-600: Paystub analysis endpoint implementation
Lines 601-800: Money order analysis endpoint implementation
Lines 801-1000: Bank statement analysis endpoint implementation
Lines 1001-1200: List/search endpoints for all document types
Lines 1201-1400: Real-time transaction monitoring endpoint
Lines 1401-1600: Document management and enriched data endpoints
Lines 1601-1800: Authentication endpoints
Lines 1801-2000: Health check and utility endpoints
Lines 2001-2085: Server startup and route logging


2.2 CHECK ANALYSIS MODULE
--------------------------

FILE: Backend/check/check_extractor.py
LINES: 600 lines
PURPOSE: Complete check fraud detection pipeline

ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│                     CheckExtractor Class                         │
├─────────────────────────────────────────────────────────────────┤
│  Stage 1: OCR Extraction (Mindee API)                           │
│  Stage 2: Data Normalization (Bank-specific)                    │
│  Stage 3: Validation Rules (Collect issues)                     │
│  Stage 4: ML Fraud Detection (XGBoost)                          │
│  Stage 5: Customer History & Duplicate Check                    │
│  Stage 6: AI Fraud Analysis (GPT-4)                             │
│  Stage 7: Generate Anomalies                                    │
│  Stage 8: Calculate Confidence Score                            │
│  Stage 9: Determine Final Decision                              │
│  Stage 10: Build Complete Response                              │
└─────────────────────────────────────────────────────────────────┘

KEY METHODS:

1. __init__(self):
   - Initializes ML detector (CheckFraudDetector)
   - Initializes AI agent (CheckFraudAnalysisAgent)
   - Loads environment variables for API keys

2. extract_and_analyze(image_path: str) -> Dict:
   - Main entry point for check analysis
   - Orchestrates all 10 stages
   - Returns complete analysis results
   - NO EARLY EXITS - always runs full pipeline

3. _extract_with_mindee(file_path: str) -> Tuple[Dict, str]:
   - Uses Mindee ClientV2 API
   - Extracts check fields: payer, payee, amount, check_number, etc.
   - Returns extracted data + raw OCR text
   - Field mapping for standardization

4. _normalize_data(extracted_data: Dict, bank_name: str) -> Dict:
   - Uses CheckNormalizerFactory to get bank-specific normalizer
   - Normalizes amounts, dates, names
   - Adds validation flags (is_valid, completeness_score)
   - Returns normalized data dict

5. _collect_validation_issues(data: Dict) -> List[str]:
   - Checks for missing signature
   - Validates critical fields (payer, payee, check_number)
   - Checks for same payer/payee
   - Validates amount ranges
   - Returns list of issues (doesn't exit early)

6. _run_ml_fraud_detection(data: Dict, raw_text: str) -> Dict:
   - Calls CheckFraudDetector.predict_fraud()
   - Returns fraud_risk_score, risk_level, model_confidence, anomalies
   - Handles exceptions gracefully

7. _get_customer_info(data: Dict) -> Dict:
   - Queries database for customer history
   - Returns total_submissions, high_risk_count, etc.
   - Used for repeat offender detection

8. _check_duplicate(data: Dict) -> bool:
   - Checks if check_number + payer_name already exists
   - Prevents duplicate submissions
   - Returns boolean

9. _run_ai_analysis(data, ml_analysis, customer_info) -> Optional[Dict]:
   - Calls CheckFraudAnalysisAgent.analyze_fraud()
   - Provides contextual analysis with GPT-4
   - Returns recommendation, confidence_score, summary, key_indicators

10. _determine_final_decision(...) -> str:
    - REJECT if missing signature
    - REJECT if critical issues exist
    - Defers to AI recommendation if available
    - Falls back to ML score thresholds
    - Returns: APPROVE, REJECT, or ESCALATE

ANNOTATIONS:

Lines 1-60: Imports, Mindee client initialization, environment setup
Lines 62-88: CheckExtractor class initialization
Lines 89-156: Main extract_and_analyze method (10-stage pipeline)
Lines 157-294: Mindee OCR extraction implementation
Lines 296-322: Bank-specific normalization
Lines 324-382: Validation issue collection
Lines 384-397: ML fraud detection wrapper
Lines 399-411: Customer history retrieval
Lines 413-427: Duplicate check detection
Lines 429-445: AI fraud analysis wrapper
Lines 447-477: Anomaly generation
Lines 479-498: Confidence score calculation
Lines 500-554: Final decision determination logic
Lines 556-584: Complete response builder
Lines 587-600: Legacy function for backward compatibility


2.3 PAYSTUB ANALYSIS MODULE
----------------------------

FILE: Backend/paystub/paystub_extractor.py
LINES: 537 lines
PURPOSE: Complete paystub fraud detection pipeline

SIMILAR ARCHITECTURE TO CHECK MODULE:

Key Differences:
1. Different Mindee model ID (paystub-specific)
2. Different field extraction (employee_name, gross_pay, net_pay, etc.)
3. Paystub-specific validation (net pay < gross pay)
4. Tax deduction processing (federal, state, social security, medicare)
5. Repeat offender policy: REJECT if escalate_count > 0 AND fraud_risk >= 20%

UNIQUE FEATURES:

1. Tax Deduction Extraction:
   - Processes deductions array from Mindee
   - Extracts federal_tax, state_tax, social_security, medicare
   - Handles nested objects in deductions

2. Employee History Tracking:
   - Tracks escalate_count for repeat offenders
   - Post-AI validation overrides recommendation if needed
   - Allows approval for low-risk repeat offenders (<20% fraud risk)

3. Pay Period Validation:
   - Checks for pay_period_start and pay_period_end
   - Validates date ranges
   - Duplicate detection based on pay period

ANNOTATIONS:

Lines 1-54: Imports, Mindee setup, environment configuration
Lines 56-93: PaystubExtractor initialization
Lines 94-160: Main analysis pipeline (10 stages)
Lines 162-322: Mindee extraction with tax processing
Lines 324-338: Paystub normalization
Lines 340-365: Validation (critical fields, gross vs net)
Lines 367-375: ML fraud detection
Lines 377-389: Employee history retrieval
Lines 391-408: Duplicate paystub detection
Lines 410-460: AI analysis with post-validation override
Lines 462-474: Anomaly generation
Lines 476-482: Confidence calculation
Lines 484-498: Final decision logic
Lines 500-535: Complete response builder


2.4 ML FRAUD DETECTION MODELS
------------------------------

COMMON ARCHITECTURE FOR ALL DOCUMENT TYPES:

FILE PATTERN: Backend/{document_type}/ml/{document_type}_fraud_detector.py

EXAMPLE: Backend/check/ml/check_fraud_detector.py

CLASS: CheckFraudDetector

KEY METHODS:

1. __init__(model_dir: str):
   - Loads pre-trained XGBoost model from .pkl file
   - Initializes feature engineering pipeline
   - Sets up logging

2. predict_fraud(data: Dict, raw_text: str) -> Dict:
   - Extracts features from normalized data
   - Applies feature engineering
   - Runs XGBoost prediction
   - Calculates risk score and confidence
   - Returns analysis dict

3. _extract_features(data: Dict) -> np.array:
   - Converts dict to feature vector
   - Handles missing values
   - Normalizes numerical features
   - Encodes categorical features

FEATURE ENGINEERING (50+ features):

1. Amount Features:
   - amount_normalized (0-1 scale)
   - amount_log (log transformation)
   - amount_rounded (is round number)

2. Date Features:
   - day_of_week (0-6)
   - month (1-12)
   - is_weekend (boolean)
   - days_since_epoch

3. Text Features:
   - payer_name_length
   - payee_name_length
   - bank_name_length
   - check_number_length

4. Missing Field Indicators:
   - missing_payer (boolean)
   - missing_payee (boolean)
   - missing_amount (boolean)
   - missing_date (boolean)

5. Customer Behavior:
   - total_submissions
   - high_risk_count
   - avg_previous_risk
   - is_repeat_customer

6. Document-Specific:
   For Checks: routing_number_valid, account_number_length
   For Paystubs: gross_net_ratio, tax_deduction_ratio
   For Money Orders: amount_within_limit, issuer_known
   For Bank Statements: balance_consistency, transaction_count

MODEL TRAINING:

FILE PATTERN: Backend/{document_type}/ml/train_{document_type}_model.py

PROCESS:
1. Load training dataset (CSV)
2. Split into train/test (80/20)
3. Handle imbalanced data with SMOTE
4. Train XGBoost classifier
5. Hyperparameter tuning with GridSearchCV
6. Evaluate on test set
7. Save model to .pkl file

HYPERPARAMETERS:
- max_depth: 6
- learning_rate: 0.1
- n_estimators: 100
- subsample: 0.8
- colsample_bytree: 0.8
- scale_pos_weight: (for imbalanced data)


2.5 AI FRAUD ANALYSIS AGENTS
-----------------------------

FILE PATTERN: Backend/{document_type}/ai/{document_type}_fraud_analysis_agent.py

EXAMPLE: Backend/check/ai/check_fraud_analysis_agent.py

CLASS: CheckFraudAnalysisAgent

ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│                  CheckFraudAnalysisAgent                         │
├─────────────────────────────────────────────────────────────────┤
│  1. Initialize OpenAI client (GPT-4)                            │
│  2. Load fraud detection prompts                                │
│  3. Initialize data access tools                                │
│  4. Create LangChain agent                                      │
├─────────────────────────────────────────────────────────────────┤
│  analyze_fraud(extracted_data, ml_analysis, payer_name)         │
│    ├─ Classify customer (new vs repeat)                        │
│    ├─ Build context from ML analysis                           │
│    ├─ Generate GPT-4 prompt                                    │
│    ├─ Call OpenAI API                                          │
│    ├─ Parse AI response                                        │
│    └─ Return recommendation + reasoning                        │
└─────────────────────────────────────────────────────────────────┘

KEY METHODS:

1. __init__(api_key, model, data_tools):
   - Initializes OpenAI client
   - Sets model (default: gpt-4-mini)
   - Stores data access tools reference

2. analyze_fraud(extracted_data, ml_analysis, payer_name) -> Dict:
   - Main analysis method
   - Classifies customer (new vs repeat)
   - Builds comprehensive prompt
   - Calls GPT-4 for analysis
   - Parses and returns structured response

3. _classify_customer(payer_name) -> Dict:
   - Queries database for customer history
   - Returns is_new_customer, total_submissions, high_risk_count
   - Used for fraud type determination

4. _build_prompt(extracted_data, ml_analysis, customer_info) -> str:
   - Creates detailed prompt for GPT-4
   - Includes all extracted fields
   - Includes ML risk score and anomalies
   - Includes customer history
   - Specifies output format

5. _parse_ai_response(response_text) -> Dict:
   - Extracts recommendation (APPROVE/REJECT/ESCALATE)
   - Extracts confidence_score (0.0-1.0)
   - Extracts summary (natural language)
   - Extracts key_indicators (list)
   - Extracts fraud_types (if applicable)

PROMPT ENGINEERING:

SYSTEM PROMPT:
"You are an expert fraud detection analyst specializing in financial 
documents. Analyze the provided check data and ML analysis results to 
determine if the check is fraudulent."

USER PROMPT STRUCTURE:
1. Document Data (all extracted fields)
2. ML Analysis (risk score, confidence, anomalies)
3. Customer History (new vs repeat, submission count)
4. Instructions (what to analyze, output format)

OUTPUT FORMAT:
{
  "recommendation": "APPROVE|REJECT|ESCALATE",
  "confidence_score": 0.0-1.0,
  "summary": "Natural language explanation",
  "key_indicators": ["indicator1", "indicator2", ...],
  "fraud_types": ["type1", "type2", ...],  // Only for REJECT/ESCALATE
  "reasoning": ["reason1", "reason2", ...]
}

FRAUD TYPE CLASSIFICATION RULES:

For NEW CUSTOMERS:
- Never show fraud types (even if REJECT/ESCALATE)
- Recommendation based purely on risk score

For REPEAT CUSTOMERS:
- Show fraud types only for REJECT/ESCALATE
- Never show fraud types for APPROVE

Fraud Types:
1. Fabricated Document - Low confidence, missing fields
2. Altered Legitimate Document - Tampering signs
3. Suspicious Transaction Patterns - Unusual activity
4. Balance Consistency Violation - Math errors
5. Unrealistic Financial Proportion - Improbable values
6. Repeat Offender - 2+ high-risk submissions


2.6 DATA ACCESS TOOLS
----------------------

FILE PATTERN: Backend/{document_type}/ai/{document_type}_tools.py

EXAMPLE: Backend/check/ai/check_tools.py

CLASS: CheckDataAccessTools

PURPOSE: Provide database access methods for AI agents

KEY METHODS:

1. get_customer_history(payer_name: str) -> Dict:
   - Queries Supabase for customer history
   - Returns total_submissions, high_risk_count, last_submission_date
   - Used for repeat offender detection

2. check_duplicate(check_number: str, payer_name: str) -> bool:
   - Queries database for existing check
   - Returns True if duplicate found
   - Prevents duplicate submissions

3. get_bank_info(bank_name: str) -> Dict:
   - Queries bank_dictionary table
   - Returns bank details (routing_number, address, etc.)
   - Used for bank validation

4. get_recent_checks(payer_name: str, limit: int = 10) -> List[Dict]:
   - Retrieves recent check submissions
   - Used for pattern analysis
   - Returns list of check records

DATABASE INTEGRATION:

Uses Supabase client from database/supabase_client.py:

from database.supabase_client import get_supabase

supabase = get_supabase()
response = supabase.table('checks').select('*').eq('payer_name', name).execute()


2.7 NORMALIZATION MODULES
--------------------------

FILE PATTERN: Backend/{document_type}/normalization/

ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│              Normalizer Factory Pattern                          │
├─────────────────────────────────────────────────────────────────┤
│  CheckNormalizerFactory.get_normalizer(bank_name)               │
│    ├─ Bank of America → BOACheckNormalizer                     │
│    ├─ Chase → ChaseCheckNormalizer                             │
│    ├─ Wells Fargo → WellsFargoCheckNormalizer                  │
│    └─ Default → GenericCheckNormalizer                         │
└─────────────────────────────────────────────────────────────────┘

BASE CLASS: CheckNormalizer

METHODS:
1. normalize(extracted_data: Dict) -> NormalizedCheck
2. is_valid() -> bool
3. get_completeness_score() -> float
4. get_critical_missing_fields() -> List[str]
5. to_dict() -> Dict

BANK-SPECIFIC NORMALIZERS:

Each bank has unique field names and formats:

Bank of America:
- Amount field: "AMOUNT"
- Date format: "MM/DD/YYYY"
- Check number: "CHECK NO."

Chase:
- Amount field: "PAY"
- Date format: "MM-DD-YYYY"
- Check number: "CHECK #"

Wells Fargo:
- Amount field: "DOLLARS"
- Date format: "MM/DD/YY"
- Check number: "CHECK NUMBER"

NORMALIZATION PROCESS:

1. Extract raw fields from Mindee response
2. Map to standard field names
3. Parse and validate amounts (remove $, commas)
4. Parse and validate dates (convert to ISO format)
5. Validate required fields
6. Calculate completeness score
7. Return NormalizedCheck object

EXAMPLE:

Raw Mindee Data:
{
  "AMOUNT": "$1,234.56",
  "DATE": "12/05/2024",
  "CHECK NO.": "1001",
  "PAY TO": "John Doe"
}

Normalized Data:
{
  "amount_numeric": 1234.56,
  "date": "2024-12-05",
  "check_number": "1001",
  "payee_name": "John Doe",
  "is_valid": True,
  "completeness_score": 0.85
}


2.8 DATABASE STORAGE MODULES
-----------------------------

FILE PATTERN: Backend/database/document_storage.py

FUNCTIONS:

1. store_check_analysis(document_id, analysis_results) -> Dict:
   - Inserts check record into 'checks' table
   - Stores extracted data, ML results, AI recommendation
   - Updates customer_history table
   - Returns stored record

2. store_paystub_analysis(document_id, analysis_results) -> Dict:
   - Inserts paystub record into 'paystubs' table
   - Similar to check storage
   - Returns stored record

3. store_money_order_analysis(document_id, analysis_results) -> Dict:
   - Inserts money order record
   - Returns stored record

4. store_bank_statement_analysis(document_id, analysis_results) -> Dict:
   - Inserts bank statement record
   - Returns stored record

SUPABASE CLIENT:

FILE: Backend/database/supabase_client.py

FUNCTION: get_supabase() -> Client

IMPLEMENTATION:
```python
from supabase import create_client

SUPABASE_URL = os.getenv("SUPABASE_URL")
SUPABASE_KEY = os.getenv("SUPABASE_KEY")

supabase = create_client(SUPABASE_URL, SUPABASE_KEY)
return supabase
```

OPERATIONS:

Insert:
supabase.table('checks').insert(data).execute()

Select:
supabase.table('checks').select('*').eq('check_id', id).execute()

Update:
supabase.table('checks').update(data).eq('check_id', id).execute()

Delete:
supabase.table('checks').delete().eq('check_id', id).execute()

================================================================================
                    3. FRONTEND ARCHITECTURE & COMPONENTS
================================================================================

3.1 APPLICATION STRUCTURE
--------------------------

FILE: Frontend/src/App.js
LINES: 87 lines

ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│                         App Component                            │
├─────────────────────────────────────────────────────────────────┤
│  AuthProvider (Context)                                         │
│    └─ Router (React Router)                                     │
│         └─ AppContent                                           │
│              ├─ Header (conditional)                            │
│              ├─ Breadcrumb (conditional)                        │
│              ├─ Routes                                          │
│              │    ├─ / → LandingPage                           │
│              │    ├─ /login → LoginPage                        │
│              │    ├─ /register → RegisterPage                  │
│              │    ├─ /check-analysis → CheckAnalysis           │
│              │    ├─ /paystub-analysis → PaystubAnalysis       │
│              │    ├─ /money-order-analysis → MoneyOrderAnalysis│
│              │    ├─ /bank-statement-analysis → BankStatement  │
│              │    └─ /real-time-analysis → RealTimeAnalysis    │
│              └─ Footer (conditional)                            │
└─────────────────────────────────────────────────────────────────┘

KEY FEATURES:

1. Conditional Layout:
   - Landing, login, register pages: No header/footer
   - Analysis pages: Full layout with header, breadcrumb, footer

2. Protected Routes:
   - Uses ProtectedRoute component
   - Checks JWT token in localStorage
   - Redirects to login if not authenticated

3. Context Management:
   - AuthContext provides user state globally
   - Accessible via useAuth() hook

ANNOTATIONS:

Lines 1-22: Imports (React, Router, Pages, Components)
Lines 24-73: AppContent component (layout logic)
Lines 75-83: App component (providers wrapper)


3.2 ANALYSIS PAGES
------------------

FILE PATTERN: Frontend/src/pages/{DocumentType}Analysis.jsx

EXAMPLE: Frontend/src/pages/CheckAnalysis.jsx

COMPONENT STRUCTURE:

┌─────────────────────────────────────────────────────────────────┐
│                    CheckAnalysis Component                       │
├─────────────────────────────────────────────────────────────────┤
│  State Management:                                              │
│    - selectedFile (uploaded file)                               │
│    - analysisResult (API response)                              │
│    - loading (boolean)                                          │
│    - error (string)                                             │
├─────────────────────────────────────────────────────────────────┤
│  UI Sections:                                                   │
│    1. File Upload (Dropzone)                                    │
│    2. Loading Indicator                                         │
│    3. Error Display                                             │
│    4. Results Display                                           │
│         ├─ Risk Score Card                                      │
│         ├─ AI Recommendation Badge                              │
│         ├─ Extracted Data Table                                 │
│         ├─ Anomalies List                                       │
│         └─ Detailed Analysis                                    │
└─────────────────────────────────────────────────────────────────┘

KEY FUNCTIONS:

1. handleFileSelect(file):
   - Called when user drops/selects file
   - Validates file type (PDF/image)
   - Sets selectedFile state
   - Triggers analysis

2. analyzeDocument(file):
   - Creates FormData with file
   - POSTs to /api/check/analyze
   - Handles loading state
   - Stores result in state
   - Displays error if fails

3. renderResults():
   - Displays risk score with color coding
   - Shows AI recommendation badge
   - Renders extracted data in table
   - Lists anomalies with icons
   - Shows detailed analysis

RISK SCORE VISUALIZATION:

Color Coding:
- 0-30%: Green (Low Risk)
- 30-70%: Yellow (Medium Risk)
- 70-100%: Red (High Risk)

Display:
- Circular progress indicator
- Percentage text
- Risk level label
- Confidence score

API INTEGRATION:

```javascript
const formData = new FormData();
formData.append('file', file);

const response = await fetch('/api/check/analyze', {
  method: 'POST',
  body: formData
});

const data = await response.json();
setAnalysisResult(data);
```


3.3 INSIGHTS DASHBOARDS
------------------------

FILE PATTERN: Frontend/src/components/{DocumentType}Insights.jsx

EXAMPLE: Frontend/src/components/BankStatementInsights.jsx
LINES: 1571 lines

COMPONENT ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│              BankStatementInsights Component                     │
├─────────────────────────────────────────────────────────────────┤
│  State Management (18 state variables):                         │
│    - csvData (processed data)                                   │
│    - loading, error                                             │
│    - bankStatementsList (raw data from API)                     │
│    - dateFilter (last_30, last_60, custom, etc.)                │
│    - customDateRange {startDate, endDate}                       │
│    - bankFilter (selected bank)                                 │
│    - searchQuery (search text)                                  │
│    - totalRecords (count)                                       │
│    - showCustomDatePicker (boolean)                             │
├─────────────────────────────────────────────────────────────────┤
│  Data Processing Functions:                                     │
│    - parseCSV() - Parse CSV text                                │
│    - processData() - Calculate metrics and charts               │
│    - fetchBankStatementsList() - API call                       │
│    - loadBankStatementData() - Transform DB records             │
├─────────────────────────────────────────────────────────────────┤
│  UI Components:                                                 │
│    1. Search Bar                                                │
│    2. Bank Filter Dropdown                                      │
│    3. Date Filter Buttons                                       │
│    4. Custom Date Range Picker                                  │
│    5. Metrics Cards (Total, Avg Risk, Counts)                  │
│    6. Pie Chart (AI Recommendations)                            │
│    7. Bar Chart (Risk Distribution)                             │
│    8. Line Chart (Fraud Trend Over Time)                        │
│    9. Bar Chart (Risk by Bank)                                  │
│    10. Pie Chart (Fraud Type Distribution)                      │
└─────────────────────────────────────────────────────────────────┘

DATA PROCESSING PIPELINE:

1. fetchBankStatementsList(filter, bank, customRange):
   - Builds query parameters
   - Calls /api/bank-statements/list
   - Stores raw data in state
   - Triggers data processing

2. loadBankStatementData(bankStatements):
   - Transforms database records to chart format
   - Parses JSONB fields (top_anomalies)
   - Calls processData()

3. processData(rows):
   - Calculates metrics (total, avg risk, counts)
   - Generates risk distribution (0-25%, 25-50%, etc.)
   - Generates recommendation distribution (APPROVE/REJECT/ESCALATE)
   - Generates fraud type distribution
   - Generates trend over time
   - Generates risk by bank
   - Returns processed data object

CHART CONFIGURATIONS:

Pie Chart (Recommendations):
- Data: {name: 'APPROVE', value: count}
- Colors: Green (APPROVE), Red (REJECT), Yellow (ESCALATE)
- Interactive: Click to filter

Bar Chart (Risk Distribution):
- X-axis: Risk ranges (0-25%, 25-50%, 50-75%, 75-100%)
- Y-axis: Count
- Color: Gradient from green to red

Line Chart (Trend):
- X-axis: Date
- Y-axis: Average risk score
- Line: Smooth curve
- Points: High-risk count

CUSTOM DATE RANGE FEATURE:

UI:
- Collapsible date picker panel
- Start date input (type="date")
- End date input (type="date")
- Apply button
- Cancel button

Validation:
- At least one date required
- Start date must be before end date
- Shows error message if invalid

API Integration:
```javascript
const params = new URLSearchParams();
if (customRange.startDate) params.append('start_date', customRange.startDate);
if (customRange.endDate) params.append('end_date', customRange.endDate);

const url = `/api/bank-statements/list?${params.toString()}`;
```

ANNOTATIONS:

Lines 1-51: Imports and state initialization
Lines 53-90: CSV parsing utility
Lines 92-410: Data processing logic (metrics, charts)
Lines 413-456: File upload handler (legacy, not used)
Lines 458-522: API data fetching with filters
Lines 524-607: Database record transformation
Lines 609-614: Auto-fetch on component mount
Lines 668-1433: UI rendering (filters, charts, metrics)


3.4 AUTHENTICATION CONTEXT
---------------------------

FILE: Frontend/src/context/AuthContext.js

PURPOSE: Global authentication state management

ARCHITECTURE:

┌─────────────────────────────────────────────────────────────────┐
│                      AuthContext                                 │
├─────────────────────────────────────────────────────────────────┤
│  State:                                                         │
│    - user (object or null)                                      │
│    - token (JWT string or null)                                 │
│    - loading (boolean)                                          │
├─────────────────────────────────────────────────────────────────┤
│  Methods:                                                       │
│    - login(email, password)                                     │
│    - register(email, password, name)                            │
│    - logout()                                                   │
│    - checkAuth()                                                │
└─────────────────────────────────────────────────────────────────┘

IMPLEMENTATION:

```javascript
const AuthContext = createContext();

export const AuthProvider = ({ children }) => {
  const [user, setUser] = useState(null);
  const [token, setToken] = useState(localStorage.getItem('token'));
  const [loading, setLoading] = useState(true);

  useEffect(() => {
    checkAuth();
  }, []);

  const checkAuth = async () => {
    const token = localStorage.getItem('token');
    if (token) {
      // Verify token with backend
      const response = await fetch('/api/auth/verify', {
        headers: { 'Authorization': `Bearer ${token}` }
      });
      if (response.ok) {
        const data = await response.json();
        setUser(data.user);
      } else {
        logout();
      }
    }
    setLoading(false);
  };

  const login = async (email, password) => {
    const response = await fetch('/api/auth/login', {
      method: 'POST',
      headers: { 'Content-Type': 'application/json' },
      body: JSON.stringify({ email, password })
    });
    const data = await response.json();
    if (data.success) {
      localStorage.setItem('token', data.token);
      setToken(data.token);
      setUser(data.user);
    }
    return data;
  };

  const logout = () => {
    localStorage.removeItem('token');
    setToken(null);
    setUser(null);
  };

  return (
    <AuthContext.Provider value={{ user, token, login, logout, loading }}>
      {children}
    </AuthContext.Provider>
  );
};

export const useAuth = () => useContext(AuthContext);
```

USAGE IN COMPONENTS:

```javascript
import { useAuth } from '../context/AuthContext';

function MyComponent() {
  const { user, login, logout } = useAuth();

  if (!user) {
    return <LoginForm onLogin={login} />;
  }

  return <div>Welcome, {user.name}!</div>;
}
```


3.5 PROTECTED ROUTE COMPONENT
------------------------------

FILE: Frontend/src/components/ProtectedRoute.jsx

PURPOSE: Restrict access to authenticated users only

IMPLEMENTATION:

```javascript
import { Navigate } from 'react-router-dom';
import { useAuth } from '../context/AuthContext';

const ProtectedRoute = ({ children }) => {
  const { user, loading } = useAuth();

  if (loading) {
    return <div>Loading...</div>;
  }

  if (!user) {
    return <Navigate to="/login" replace />;
  }

  return children;
};

export default ProtectedRoute;
```

USAGE:

```javascript
<Route 
  path="/finance" 
  element={
    <ProtectedRoute>
      <HomePage />
    </ProtectedRoute>
  } 
/>
```

================================================================================
                    4. DATA FLOW & PROCESSING PIPELINE
================================================================================

4.1 COMPLETE DOCUMENT ANALYSIS FLOW
------------------------------------

USER ACTION: Upload check image

STEP 1: FRONTEND (CheckAnalysis.jsx)
┌─────────────────────────────────────────────────────────────────┐
│  1. User drops file on dropzone                                 │
│  2. handleFileSelect() validates file type                      │
│  3. analyzeDocument() creates FormData                          │
│  4. POST /api/check/analyze with multipart/form-data            │
│  5. Set loading=true, show spinner                              │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 2: BACKEND API (api_server.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Receive file upload request                                 │
│  2. Validate file extension (.pdf, .jpg, .png)                  │
│  3. Generate unique filename with UUID                          │
│  4. Save file to temp_uploads/ directory                        │
│  5. Call CheckExtractor.extract_and_analyze(filepath)           │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 3: CHECK EXTRACTOR (check_extractor.py)
┌─────────────────────────────────────────────────────────────────┐
│  Stage 1: OCR Extraction                                        │
│    - Call Mindee API with file path                            │
│    - Extract fields: payer, payee, amount, check_number, etc.  │
│    - Return extracted_data dict + raw_text                     │
│                                                                 │
│  Stage 2: Normalization                                        │
│    - Detect bank name from extracted data                      │
│    - Get bank-specific normalizer from factory                 │
│    - Normalize amounts, dates, names                           │
│    - Return normalized_data dict                               │
│                                                                 │
│  Stage 3: Validation                                           │
│    - Check for missing signature                               │
│    - Validate critical fields                                  │
│    - Check for duplicate submission                            │
│    - Collect validation_issues list                            │
│                                                                 │
│  Stage 4: ML Fraud Detection                                   │
│    - Extract 50+ features from normalized data                 │
│    - Run XGBoost model prediction                              │
│    - Calculate fraud_risk_score (0.0-1.0)                      │
│    - Determine risk_level (LOW/MEDIUM/HIGH/CRITICAL)           │
│    - Return ml_analysis dict                                   │
│                                                                 │
│  Stage 5: Customer History                                     │
│    - Query database for payer_name history                     │
│    - Get total_submissions, high_risk_count                    │
│    - Check for duplicate check_number                          │
│    - Return customer_info dict                                 │
│                                                                 │
│  Stage 6: AI Analysis                                          │
│    - Build comprehensive prompt for GPT-4                      │
│    - Include extracted data, ML results, customer history      │
│    - Call OpenAI API                                           │
│    - Parse AI response                                         │
│    - Return ai_analysis dict with recommendation               │
│                                                                 │
│  Stage 7: Anomaly Generation                                   │
│    - Combine ML anomalies + AI key_indicators                  │
│    - Add validation issues                                     │
│    - Return anomalies list                                     │
│                                                                 │
│  Stage 8: Confidence Calculation                               │
│    - Weight: 30% data completeness                             │
│    - Weight: 30% ML confidence                                 │
│    - Weight: 40% AI confidence                                 │
│    - Return confidence_score (0.0-1.0)                         │
│                                                                 │
│  Stage 9: Final Decision                                       │
│    - REJECT if missing signature                               │
│    - REJECT if critical validation issues                      │
│    - Defer to AI recommendation if available                   │
│    - Fall back to ML score thresholds                          │
│    - Return overall_decision (APPROVE/REJECT/ESCALATE)         │
│                                                                 │
│  Stage 10: Build Response                                      │
│    - Combine all analysis results                              │
│    - Add timestamp                                             │
│    - Return complete_response dict                             │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 4: DATABASE STORAGE (document_storage.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Generate document_id (UUID)                                 │
│  2. Insert into 'documents' table (master record)               │
│  3. Insert into 'checks' table (check-specific data)            │
│  4. Update 'customer_history' table                             │
│  5. Return stored record                                        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 5: API RESPONSE (api_server.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Build JSON response                                         │
│  2. Include fraud_risk_score, ai_recommendation, etc.           │
│  3. Return HTTP 200 with JSON body                              │
│  4. Clean up temp file                                          │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 6: FRONTEND DISPLAY (CheckAnalysis.jsx)
┌─────────────────────────────────────────────────────────────────┐
│  1. Receive JSON response                                       │
│  2. Set analysisResult state                                    │
│  3. Set loading=false                                           │
│  4. Render results:                                             │
│     - Risk score card with color coding                         │
│     - AI recommendation badge                                   │
│     - Extracted data table                                      │
│     - Anomalies list                                            │
│     - Detailed analysis sections                                │
└─────────────────────────────────────────────────────────────────┘

TOTAL TIME: 5-15 seconds (depending on document complexity)


4.2 INSIGHTS DASHBOARD DATA FLOW
---------------------------------

USER ACTION: Navigate to Bank Statement Insights

STEP 1: COMPONENT MOUNT (BankStatementInsights.jsx)
┌─────────────────────────────────────────────────────────────────┐
│  1. useEffect() triggers on mount                               │
│  2. Call fetchBankStatementsList()                              │
│  3. Set loading=true                                            │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 2: API REQUEST
┌─────────────────────────────────────────────────────────────────┐
│  1. Build URL: /api/bank-statements/list                        │
│  2. Add query params if filters applied                         │
│  3. Send GET request                                            │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 3: BACKEND PROCESSING (api_server.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Connect to Supabase                                         │
│  2. Query bank_statements table                                 │
│  3. Apply pagination (1000 records per page)                    │
│  4. Apply date filter if specified:                             │
│     - last_30: created_at >= now - 30 days                      │
│     - last_60: created_at >= now - 60 days                      │
│     - custom: created_at between start_date and end_date        │
│  5. Return JSON response with data array                        │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 4: DATA TRANSFORMATION (BankStatementInsights.jsx)
┌─────────────────────────────────────────────────────────────────┐
│  1. Receive data array from API                                 │
│  2. Transform database records:                                 │
│     - Parse JSONB fields (top_anomalies)                        │
│     - Convert fraud_risk_score to percentage                    │
│     - Map field names to expected format                        │
│  3. Call processData(rows)                                      │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 5: DATA PROCESSING
┌─────────────────────────────────────────────────────────────────┐
│  Calculate Metrics:                                             │
│    - totalStatements = rows.length                              │
│    - avgRiskScore = sum(risk_scores) / count                    │
│    - approveCount = count(recommendation == 'APPROVE')          │
│    - rejectCount = count(recommendation == 'REJECT')            │
│    - escalateCount = count(recommendation == 'ESCALATE')        │
│                                                                 │
│  Generate Risk Distribution:                                    │
│    - 0-25%: count(risk < 25)                                    │
│    - 25-50%: count(risk >= 25 && risk < 50)                     │
│    - 50-75%: count(risk >= 50 && risk < 75)                     │
│    - 75-100%: count(risk >= 75)                                 │
│                                                                 │
│  Generate Recommendation Distribution:                          │
│    - APPROVE: count, percentage                                 │
│    - REJECT: count, percentage                                  │
│    - ESCALATE: count, percentage                                │
│                                                                 │
│  Generate Fraud Type Distribution:                              │
│    - Group by fraud_type                                        │
│    - Count occurrences                                          │
│    - Sort by count descending                                   │
│                                                                 │
│  Generate Trend Over Time:                                      │
│    - Group by date                                              │
│    - Calculate avg risk per date                                │
│    - Count high-risk per date                                   │
│    - Sort by date ascending                                     │
│                                                                 │
│  Generate Risk by Bank:                                         │
│    - Group by bank_name                                         │
│    - Calculate avg risk per bank                                │
│    - Sort by avg risk descending                                │
│                                                                 │
│  Return processed data object                                   │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 6: UI RENDERING
┌─────────────────────────────────────────────────────────────────┐
│  1. Set csvData state with processed data                       │
│  2. Set loading=false                                           │
│  3. Render components:                                          │
│     - Metrics cards (animated counters)                         │
│     - Pie chart (recommendations)                               │
│     - Bar chart (risk distribution)                             │
│     - Line chart (trend over time)                              │
│     - Bar chart (risk by bank)                                  │
│     - Pie chart (fraud types)                                   │
│  4. Enable interactive features (click to filter)               │
└─────────────────────────────────────────────────────────────────┘

TOTAL TIME: 1-3 seconds


4.3 REAL-TIME TRANSACTION MONITORING FLOW
------------------------------------------

USER ACTION: Upload transaction CSV

STEP 1: FRONTEND (RealTimeAnalysis.jsx)
┌─────────────────────────────────────────────────────────────────┐
│  1. User uploads CSV file                                       │
│  2. Validate file type (.csv)                                   │
│  3. Create FormData                                             │
│  4. POST /api/real-time/analyze                                 │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 2: BACKEND API (api_server.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Receive CSV file                                            │
│  2. Save to temp directory                                      │
│  3. Call real_time.process_transaction_csv(filepath)            │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 3: CSV PROCESSING (real_time/csv_processor.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Read CSV file with pandas                                   │
│  2. Validate required columns:                                  │
│     - transaction_id, timestamp, amount, merchant               │
│     - customer_id, location, transaction_type                   │
│  3. Parse and validate data types                               │
│  4. Return transactions list                                    │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 4: FRAUD DETECTION (real_time/fraud_detector.py)
┌─────────────────────────────────────────────────────────────────┐
│  For each transaction:                                          │
│    1. Extract features:                                         │
│       - Amount (normalized, log, rounded)                       │
│       - Time features (hour, day_of_week, is_weekend)           │
│       - Merchant category                                       │
│       - Location (city, state, country)                         │
│       - Customer history (velocity, avg_amount)                 │
│                                                                 │
│    2. Run ML model prediction                                   │
│       - Calculate fraud_risk_score                              │
│       - Determine risk_level                                    │
│                                                                 │
│    3. Detect patterns:                                          │
│       - Velocity check (transactions per hour)                  │
│       - Geographic anomaly (unusual location)                   │
│       - Amount anomaly (unusual for merchant)                   │
│       - Time anomaly (odd hours)                                │
│                                                                 │
│    4. Add risk_score and flags to transaction                   │
│                                                                 │
│  Return analyzed transactions list                              │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 5: INSIGHTS GENERATION (real_time/insights_generator.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Calculate summary metrics:                                  │
│     - Total transactions                                        │
│     - High-risk count                                           │
│     - Average risk score                                        │
│     - Total amount                                              │
│                                                                 │
│  2. Generate risk distribution:                                 │
│     - Low risk (0-30%)                                          │
│     - Medium risk (30-70%)                                      │
│     - High risk (70-100%)                                       │
│                                                                 │
│  3. Generate geographic heatmap data:                           │
│     - Group by location                                         │
│     - Calculate risk per location                               │
│     - Format for map visualization                              │
│                                                                 │
│  4. Generate time-series data:                                  │
│     - Group by hour/day                                         │
│     - Calculate risk over time                                  │
│                                                                 │
│  5. Identify top high-risk transactions                         │
│                                                                 │
│  Return insights object                                         │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 6: AI ANALYSIS (real_time/agent_service.py)
┌─────────────────────────────────────────────────────────────────┐
│  1. Build comprehensive prompt:                                 │
│     - Summary metrics                                           │
│     - High-risk transactions                                    │
│     - Pattern analysis                                          │
│                                                                 │
│  2. Call GPT-4 for natural language summary                     │
│                                                                 │
│  3. Parse AI response:                                          │
│     - Overall risk assessment                                   │
│     - Key findings                                              │
│     - Recommendations                                           │
│                                                                 │
│  Return ai_analysis object                                      │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 7: API RESPONSE
┌─────────────────────────────────────────────────────────────────┐
│  Return JSON with:                                              │
│    - transactions (with risk scores)                            │
│    - insights (metrics, distributions)                          │
│    - ai_analysis (summary, recommendations)                     │
└─────────────────────────────────────────────────────────────────┘
                              ↓
STEP 8: FRONTEND DISPLAY
┌─────────────────────────────────────────────────────────────────┐
│  Render:                                                        │
│    - Summary metrics cards                                      │
│    - Transaction list with risk indicators                      │
│    - Geographic heatmap (ECharts)                               │
│    - Time-series chart                                          │
│    - Risk distribution chart                                    │
│    - AI summary panel                                           │
└─────────────────────────────────────────────────────────────────┘

================================================================================
                        END OF CODE ANNOTATIONS
================================================================================

This comprehensive annotation document covers the complete XFORIA DAD system
architecture, code organization, data flows, and implementation details.

For specific code examples, API references, or detailed function documentation,
refer to the inline code comments in each module.

Last Updated: December 8, 2024

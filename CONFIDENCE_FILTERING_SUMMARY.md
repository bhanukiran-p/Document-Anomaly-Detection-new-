# Confidence Filtering Implementation Summary

## What Was Added

### ‚úÖ Configuration Update

**File**: `Backend/training/retraining_config.json`

**Added Section**:
```json
{
  "data_quality": {
    "min_confidence_score": 0.80,
    "require_high_confidence_only": true,
    "confidence_score_field": "confidence_score",
    "min_fraud_ratio": 0.10,
    "max_fraud_ratio": 0.60
  }
}
```

### üìã Configuration Parameters Explained

| Parameter | Value | Purpose |
|-----------|-------|---------|
| `min_confidence_score` | **0.80** | Minimum confidence threshold (80%) - balanced between quality and quantity |
| `require_high_confidence_only` | **true** | Enable/disable confidence filtering (can toggle off if needed) |
| `confidence_score_field` | **"confidence_score"** | Database column name for confidence scores |
| `min_fraud_ratio` | **0.10** | Minimum 10% fraud cases required (prevents all-legitimate datasets) |
| `max_fraud_ratio` | **0.60** | Maximum 60% fraud cases allowed (prevents all-fraud datasets) |

---

## How It Works

### Database Query Logic

**WITHOUT Confidence Filtering** (old approach):
```python
# Fetches ALL documents with ai_recommendation
response = supabase.table('paystubs')\
    .select('*')\
    .in_('ai_recommendation', ['APPROVE', 'REJECT'])\
    .limit(10000)\
    .execute()

# Result: 300 documents (includes low-confidence predictions)
```

**WITH Confidence Filtering** (new approach):
```python
# Fetches ONLY high-confidence documents
response = supabase.table('paystubs')\
    .select('*')\
    .in_('ai_recommendation', ['APPROVE', 'REJECT'])\
    .gte('confidence_score', 0.80)  # ‚Üê NEW: Confidence filter
    .order('created_at', desc=True)\
    .limit(10000)\
    .execute()

# Result: 180 high-quality documents (36% filtered out, 64% retained)
```

---

## Impact on Data Blending

### Example Scenario: Week 8

**Database State**:
- Total documents: 400
- APPROVE: 260 (avg confidence: 0.75)
- REJECT: 140 (avg confidence: 0.88)

**WITHOUT Confidence Filter**:
```
Usable documents: 400 (all)
Mode: Hybrid (< 500 threshold)
Synthetic: 267 samples
Real: 400 samples
Total training: 667 samples
```

**WITH Confidence Filter (‚â•0.80)**:
```
Filtered breakdown:
- APPROVE: 260 ‚Üí 130 (50% pass filter)
- REJECT: 140 ‚Üí 120 (86% pass filter)

Usable documents: 250 (high-confidence only)
Mode: Hybrid (< 500 threshold)
Synthetic: 167 samples
Real: 250 samples
Total training: 417 samples

Data loss: 400 ‚Üí 250 (37.5% filtered out)
Quality gain: Avg confidence 0.75 ‚Üí 0.86 (+14.7%)
```

---

## Transition Timelines

### WITHOUT Confidence Filtering

| Week | Total Docs | Usable | Mode | Synthetic | Real | Total Train |
|------|------------|--------|------|-----------|------|-------------|
| 2 | 100 | 97 | Synthetic | 2000 | 0 | 2000 |
| 4 | 200 | 195 | Hybrid | 130 | 195 | 325 |
| 8 | 400 | 390 | Hybrid | 260 | 390 | 650 |
| **11** | **550** | **539** | **Real** | **0** | **539** | **539** |

**Transition to Real Mode**: Week 11 (539 usable > 500 threshold)

### WITH Confidence Filtering (‚â•0.80)

| Week | Total Docs | High-Conf | Mode | Synthetic | Real | Total Train |
|------|------------|-----------|------|-----------|------|-------------|
| 2 | 100 | 48 | Synthetic | 2000 | 0 | 2000 |
| 4 | 200 | 115 | Hybrid | 77 | 115 | 192 |
| 8 | 400 | 250 | Hybrid | 167 | 250 | 417 |
| 12 | 600 | 410 | Hybrid | 273 | 410 | 683 |
| **16** | **800** | **550** | **Real** | **0** | **550** | **550** |

**Transition to Real Mode**: Week 16 (550 high-conf > 500 threshold)

**Trade-off**: 5 weeks slower transition, but much higher label quality

---

## Expected Log Output

### During Retraining with Confidence Filtering

```
2025-01-22 14:30:52 - Starting automated retraining for paystub
2025-01-22 14:30:52 - Version ID: 20250122_143052
2025-01-22 14:30:53 - Step 1: Fetching real data from database...
2025-01-22 14:30:53 - Applying confidence filter: confidence_score >= 0.80
2025-01-22 14:30:54 - Fetched 250 high-confidence paystubs from database
2025-01-22 14:30:54 -   APPROVE: 130
2025-01-22 14:30:54 -   REJECT: 120
2025-01-22 14:30:54 -   Avg confidence: 0.863
2025-01-22 14:30:55 - Step 2: Generating synthetic data...
2025-01-22 14:30:56 - Generated 2000 synthetic samples
2025-01-22 14:30:56 - Step 3: Blending data...
2025-01-22 14:30:56 - Hybrid mode: 167 synthetic + 250 real = 417 total (40%/60% split)
2025-01-22 14:30:56 - Step 4: Validating data quality...
2025-01-22 14:30:56 - ‚úì Data quality checks passed
```

---

## Quality Metrics Comparison

### Label Quality Indicators

| Metric | Without Filter | With Filter (‚â•0.80) | Improvement |
|--------|----------------|---------------------|-------------|
| **Avg Confidence** | 0.75 | 0.86 | +14.7% |
| **Min Confidence** | 0.50 | 0.80 | +60% |
| **Label Noise (est.)** | ~25% | ~14% | 44% reduction |
| **Training Samples** | 390 | 250 | -35.9% |
| **Model Accuracy (est.)** | Baseline | +3-5% | Better generalization |

### Fraud Ratio Balance

**Without Filter**:
```
APPROVE: 260 (66.7%)
REJECT: 140 (33.3%)
Fraud ratio: 33.3% ‚úì (within 10-60% range)
```

**With Filter (‚â•0.80)**:
```
APPROVE: 130 (52.0%)
REJECT: 120 (48.0%)
Fraud ratio: 48.0% ‚úì (better balanced)
```

---

## Adjusting Confidence Threshold

### If You Have Too Much Data Loss

**Current**: `min_confidence_score: 0.80` (80%)

**Relax to**: `min_confidence_score: 0.70` (70%)
- Retains more data (~75% instead of 64%)
- Still filters out most uncertain cases
- Good compromise for high-volume systems

**Disable**: `require_high_confidence_only: false`
- Uses all data regardless of confidence
- Fastest transition to real mode
- Higher label noise risk

### If You Need Stricter Quality

**Increase to**: `min_confidence_score: 0.85` (85%)
- Only very high-confidence predictions
- Slower data accumulation (~50% retention)
- Best label quality

**Increase to**: `min_confidence_score: 0.90` (90%)
- Only near-certain predictions
- Very slow data accumulation (~30% retention)
- Highest quality, but may take months to reach real mode

---

## Testing Confidence Filtering

### Check Your Current Data Distribution

```sql
-- Run this query to see what you'd get with different thresholds
SELECT
    CASE
        WHEN confidence_score >= 0.90 THEN '0.90+'
        WHEN confidence_score >= 0.85 THEN '0.85-0.89'
        WHEN confidence_score >= 0.80 THEN '0.80-0.84'
        WHEN confidence_score >= 0.70 THEN '0.70-0.79'
        ELSE 'Below 0.70'
    END AS confidence_range,
    ai_recommendation,
    COUNT(*) as count
FROM paystubs
WHERE ai_recommendation IN ('APPROVE', 'REJECT')
GROUP BY confidence_range, ai_recommendation
ORDER BY confidence_range DESC, ai_recommendation;
```

**Expected Output**:
```
confidence_range | ai_recommendation | count
-----------------|-------------------|-------
0.90+            | APPROVE          | 45
0.90+            | REJECT           | 95
0.85-0.89        | APPROVE          | 35
0.85-0.89        | REJECT           | 15
0.80-0.84        | APPROVE          | 50     ‚Üê With 0.80: These included
0.80-0.84        | REJECT           | 10     ‚Üê With 0.80: These included
0.70-0.79        | APPROVE          | 80     ‚Üê With 0.80: These excluded
0.70-0.79        | REJECT           | 8      ‚Üê With 0.80: These excluded
Below 0.70       | APPROVE          | 60
Below 0.70       | REJECT           | 2
```

**With threshold 0.80**: 250 samples (95+15+50+10+35+45)
**With threshold 0.70**: 338 samples (adds 80+8)
**No filter**: 400 samples (all)

---

## When to Adjust the Threshold

### Lower Threshold (0.70) IF:
- ‚úÖ You have < 500 total documents after 3 months
- ‚úÖ High-confidence data is too sparse (< 100 samples)
- ‚úÖ You need faster transition to real mode
- ‚úÖ Your confidence scores are generally low (avg < 0.75)

### Keep Current (0.80) IF:
- ‚úÖ Balanced data growth (200-500 docs/month)
- ‚úÖ Model accuracy is priority over speed
- ‚úÖ Confidence scores well-distributed
- ‚úÖ You're OK with slower transition (acceptable trade-off)

### Raise Threshold (0.85) IF:
- ‚úÖ You have > 1000 total documents
- ‚úÖ Very high volume (500+ docs/month)
- ‚úÖ Label quality is critical
- ‚úÖ You can afford to be selective

---

## Configuration Change Command

### To Adjust Threshold Later

**Edit** `Backend/training/retraining_config.json`:

```json
{
  "data_quality": {
    "min_confidence_score": 0.70,  // ‚Üê Change this value
    "require_high_confidence_only": true
  }
}
```

**No code changes needed** - configuration is read at runtime.

**Effect**: Next retraining run will use new threshold.

---

## Summary

‚úÖ **Configuration Added**: Confidence filtering at 0.80 threshold
‚úÖ **Impact**: ~35-40% data filtered, but much higher quality
‚úÖ **Trade-off**: 5 weeks slower transition to real mode
‚úÖ **Flexibility**: Easy to adjust threshold or disable entirely
‚úÖ **Protection**: Prevents garbage-in-garbage-out problem
‚úÖ **Safety Net**: Automatic rollback still active (15% drop threshold)

**Recommendation**: Start with 0.80, monitor results for 4-6 weeks, adjust if needed.
